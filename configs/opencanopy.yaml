# lightning.pytorch==2.1.1
seed_everything: 0
trainer:
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: bf16
  logger:
    class_path: TensorBoardLogger
    init_args:
      save_dir: experiments
      name: opencanopy
  callbacks:
    - class_path: TQDMProgressBar
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: epoch

  max_epochs: 20
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  enable_checkpointing: true
  default_root_dir: experiments
data:
  class_path: eosfm.datamodules.OpenCanopyDataModule
  init_args:
    batch_size: 32
    num_workers: 16
  dict_kwargs:
    root_dir: data

model:
  class_path: terratorch.tasks.PixelwiseRegressionTask
  init_args:
    model_factory: EncoderDecoderFactory
    model_args:
      backbone: timm_convnextv2_atto
      backbone_pretrained: True
      decoder: UperNetDecoder
      decoder_channels: 128
      head_channel_list: [128]
      head_dropout: 0.1
    loss: mse
    ignore_index: -1
    freeze_backbone: false
    freeze_decoder: false 
    plot_on_val: false

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 1.e-4
    weight_decay: 0.1
# lr_scheduler:
#   class_path: ReduceLROnPlateau
#   init_args:
#     monitor: val/loss